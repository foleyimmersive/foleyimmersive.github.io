<!DOCTYPE html>
<html lang="zh - CN">

<head>
  <meta charset="UTF - 8">
  <meta name="viewport" content="width=device-width, initial - scale=1.0">
  <link rel="stylesheet" href="styles.css">


  <title>Foley Immersive</title>
</head>



<body>
  <!-- Ê†áÈ¢òÂíå‰Ωú1ËÄÖ‰ø°ÊÅØÈÉ®ÂàÜ -->
  <header>
    <br>
    <h1 style="text-align: center;font-size:48px;">FoleyImmersive: Decoupling What and Where <br>for Video-to-First-Order Ambisonics</h1>
  </header>

  <div class="author">
<!--     <p>Liming Liang<sup>1</sup></p>
    <p><sup>1</sup>Peking University</p> -->
    <p>(Icassp 2026)</p>
  </div> 
  <br>
  <div class="button-group">
    <a href="#" class="button">PAPER</a>
    <a href="#" class="button">CODE (COMING SOON)</a>
  </div>

  <!-- ÂÜÖÂÆπÁÆÄ‰ªãÈÉ®ÂàÜ -->
  <!-- <section class="description">
    <p>Index Terms: Video-to-Audio Generation, Dataset Construction, Semantic and Temporal Alignment, Prompt Learning.</p>
  </section> -->

  <section class="abs">
    <h2 style="text-align: center;">Abstract</h2>
    <hr>
    <p>We study immersive video-to-audio (V2A) by generating first-order ambisonics (FOA) from silent field-of-view videos. Existing immersive V2A systems struggle with (i) sparse semantics in public video‚ÄìFOA corpora, and (ii) content‚Äìgeom\-etry entanglement: end-to-end models blur ‚Äúwhat‚Äù and ‚Äúwhere‚Äù, while naive two-stage pipelines trade semantic fidelity for spatial coherence. We present FoleyImmersive, a decoupled, two-stage framework and a semantics-augmented dataset YT-AmbiSem, which enriches all YT-Ambigen clips with structured descriptions using Qwen2.5-VL. FoleyImmersive decouples what and where into two stages: Stage 1 uses a semantics-first diffusion model with multi-rate cross-frame attention and a probabilistic time modulator to produce a well-semantic mono $W$. Stage 2 spatializes $W$ to $XYZ$ with a complex-STFT U-Net conditioned on per-frame visuals and camera direction, where a lightweight directional residual mixer applies view-dependent gating at the bottleneck to stabilize localization with minimal content drift.
      Our method achieves state-of-the-art semantic and spatial metrics.</p>
    <!-- <ul>
        <li>inadequate textual annotations in existing datasets</li>
        <li>over-reliance on global video features</li>
        <li>coarse temporal synchronization. To address these, we propose FoleyMaster with three key innovations</li>
    </ul>
    <p>To address these, we propose FoleyMaster with three key innovations:</p>
    <ul>
        <li>We introduce VGGSound Plus dataset with 197,955 videos annotated by Qwen2-VL-7B for fine-grained event descriptions</li>
        <li>We develop a cross-attention semantic adapter integrating token-level text embeddings with global video features via prompt learning, enabling precise alignment between visual events and sound</li>
        <li>We develop a probabilistic temporal adapter that adjusts audio generation based on action prominence replacing binary synchronization.</li>
    </ul> -->
    
  </section>

  <!-- model structure -->
  <section class="modelstructure">
    <h2 style="text-align: center;">Model structure</h2>
    <hr>
    <br><br>
    <div class="image-struct">
      <img src="image/foleyIMS5_01.jpg" alt="Á§∫‰æãÂõæÁâá">
  </div>


  </section>

  <!-- contribution -->
  <section class="contribution">
    <h2 style="text-align: center;">Contribution</h2>
    <hr>
    <br>
    <ul>
      <li>Dataset: A semantics-augmented FOA dataset with structured textual descriptions (events, objects, environment) to enhance audio-visual grounding.</li>
      <li>Decoupled Two-Stage Architecture:
        Stage 1 (What): A semantics-first diffusion model with Multi-Rate Cross-Frame Attention (MR-CFA) and Probabilistic Time Modulation (PTM) for mono generation.
        Stage 2 (Where): A complex-STFT spatializer with a Directional Residual Mixer (DRM) for minimal-interference spatialization.</li>
      <li>SOTA Results: Our method achieves state-of-the-art performance on both semantic and spatial metrics in the Video-to-First-Order Ambisonics task.</li>
  </ul>
    
  </div>
  </section>



  <br><br><br>




  

<div class="intro" style="text-align: center;"></div>
    <h2 class="chapter-title">Comparisons with Baselines</h2>
    <hr>

    <br>
</div>

<div id="video-selector">


</div>

<!-- <script>
    const sections = [];

    function generateUI() {
        const container = document.getElementById('video-selector');
        container.innerHTML = "";

        sections.forEach(section => {
            const sectionDiv = document.createElement('div');
            sectionDiv.className = 'section';

            // Section title
            const sectionTitle = document.createElement('div');
            sectionTitle.className = 'section-title';
            sectionTitle.innerText = section.title;
            sectionDiv.appendChild(sectionTitle);

            // Each case = one row
            section.cases.forEach(videoCase => {
                const videoRow = document.createElement('div');
                videoRow.className = 'video-row';

                // ---------- Ground Truth ----------
                const gtColumn = createVideoColumn(
                    createVideoPlayer(videoCase.gt),
                    'Ground Truth'
                );
                videoRow.appendChild(gtColumn);

                // ---------- Generated methods ----------
                videoCase.generated.forEach(prefix => {
                    const videoPath = `${prefix}${videoCase.caption}.webm`;
                    const title = getGeneratedTitle(prefix);

                    const column = createVideoColumn(
                        createVideoPlayer(videoPath),
                        title
                    );
                    videoRow.appendChild(column);
                });

                sectionDiv.appendChild(videoRow);

                // Caption
                const captionText = document.createElement('div');
                captionText.className = 'caption';
                captionText.innerText = 'Video ID: ' + videoCase.caption;
                sectionDiv.appendChild(captionText);
            });

            container.appendChild(sectionDiv);
        });
    }

    function createVideoColumn(player, titleText) {
        const column = document.createElement('div');
        column.className = 'video-column';

        const title = document.createElement('div');
        title.className = 'video-title';
        title.innerText = titleText;

        column.appendChild(player);
        column.appendChild(title);
        return column;
    }

    function createVideoPlayer(videoPath) {
        const playerContainer = document.createElement('div');
        playerContainer.className = 'video-player-container';

        const iframe = document.createElement('iframe');
        iframe.src = `demo-player.html?videoPath=${encodeURIComponent(videoPath)}`;
        iframe.allowFullscreen = true;

        const fullscreenButton = document.createElement('button');
        fullscreenButton.className = 'fullscreen-btn';
        fullscreenButton.innerText = '‚õ∂';
        fullscreenButton.onclick = () => toggleFullscreen(playerContainer);

        playerContainer.appendChild(iframe);
        playerContainer.appendChild(fullscreenButton);

        return playerContainer;
    }

    function toggleFullscreen(container) {
        if (!document.fullscreenElement) {
            container.requestFullscreen();
        } else {
            document.exitFullscreen();
        }
    }

    function getGeneratedTitle(prefix) {
        if (prefix === 'videos_gen/') return 'Ours';
        if (prefix === 'videos_diff/') return 'Diff-foley+Ambi Enc';
        if (prefix === 'videos_spec/') return 'SpecVQGAN+Ambi Enc';
        if (prefix === 'videos_visage/') return 'ViSAGe';
        return '';
    }

    fetch('sections.json')
        .then(res => res.json())
        .then(jsonData => {
            for (const title in jsonData) {
                sections.push({
                    title,
                    cases: jsonData[title].map(caption => ({
                        gt: `videos_gt/${caption}.webm`,
                        generated: [
                            'videos_gen/',
                            'videos_diff/',
                            'videos_spec/',
                            'videos_visage/' // Á¨¨‰∫î‰∏™ÊñπÊ≥ï
                        ],
                        caption
                    }))
                });
            }
            generateUI();
        })
        .catch(err => console.error(err));
</script> -->
<script>
/* ===============================
   ÂÖ®Â±ÄÊï∞ÊçÆ
================================ */

const sections = [];

/* ===============================
   UI ÁîüÊàê
================================ */

function generateUI() {
    const container = document.getElementById('video-selector');
    container.innerHTML = "";

    sections.forEach(section => {
        const sectionDiv = document.createElement('div');
        sectionDiv.className = 'section';

        const sectionTitle = document.createElement('div');
        sectionTitle.className = 'section-title';
        sectionTitle.innerText = section.title;
        sectionDiv.appendChild(sectionTitle);

        section.cases.forEach(videoCase => {
            const videoRow = document.createElement('div');
            videoRow.className = 'video-row';

            // Ground Truth
            videoRow.appendChild(
                createVideoColumn(
                    createPlayerContainer(videoCase.gt),
                    'Ground Truth'
                )
            );

            // Generated methods
            videoCase.generated.forEach(prefix => {
                const videoPath = `${prefix}${videoCase.caption}.webm`;
                const title = getGeneratedTitle(prefix);

                videoRow.appendChild(
                    createVideoColumn(
                        createPlayerContainer(videoPath),
                        title
                    )
                );
            });

            sectionDiv.appendChild(videoRow);

            const captionText = document.createElement('div');
            captionText.className = 'caption';
            captionText.innerText = 'Video ID: ' + videoCase.caption;
            sectionDiv.appendChild(captionText);
        });

        container.appendChild(sectionDiv);
    });

    setupRowReload();
}

/* ===============================
   Column
================================ */

function createVideoColumn(player, titleText) {
    const column = document.createElement('div');
    column.className = 'video-column';

    const title = document.createElement('div');
    title.className = 'video-title';
    title.innerText = titleText;

    column.appendChild(player);
    column.appendChild(title);
    return column;
}

/* ===============================
   Player ÂÆπÂô®Ôºà‰∏çÁõ¥Êé•ÂàõÂª∫ iframeÔºâ
================================ */

function createPlayerContainer(videoPath) {
    const container = document.createElement('div');
    container.className = 'video-player-container';

    container.dataset.src =
        `demo-player.html?videoPath=${encodeURIComponent(videoPath)}`;

    renderPlaceholder(container);
    return container;
}

function renderPlaceholder(container) {
    container.innerHTML = `
        <div class="video-placeholder"
             style="
                height:220px;
                background:#f2f2f2;
                display:flex;
                align-items:center;
                justify-content:center;
                color:#777;
                font-size:14px;">
            Scroll to load
        </div>
    `;
}

/* ===============================
   üî• Ë°åÁ∫ß iframe ÈáçÂª∫ÔºàÊ†∏ÂøÉÔºâ
================================ */

function reloadRow(videoRow) {
    const players = videoRow.querySelectorAll('.video-player-container');

    players.forEach(container => {
        const src = container.dataset.src;

        // ÈîÄÊØÅÊóß iframe / canvas
        container.innerHTML = '';

        // ÂàõÂª∫ÂÖ®Êñ∞ iframe
        const iframe = document.createElement('iframe');
        iframe.src = src;
        iframe.allowFullscreen = true;
        iframe.loading = 'eager';
        iframe.style.cssText = `
            width:100%;
            height:220px;
            border:none;
            background:black;
        `;

        container.appendChild(iframe);
    });
}

/* ===============================
   IntersectionObserverÔºöÁõëÂê¨Êï¥Ë°å
================================ */

function setupRowReload() {
    const rows = document.querySelectorAll('.video-row');

    const observer = new IntersectionObserver(entries => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                reloadRow(entry.target);
            }
        });
    }, {
        rootMargin: '150px',
        threshold: 0.2
    });

    rows.forEach(row => observer.observe(row));
}

/* ===============================
   Title mapping
================================ */

function getGeneratedTitle(prefix) {
    if (prefix === 'videos_gen/') return 'Ours';
    if (prefix === 'videos_diff/') return 'Diff-Foley + Ambi Enc';
    if (prefix === 'videos_spec/') return 'SpecVQGAN + Ambi Enc';
    if (prefix === 'videos_visage/') return 'ViSAGe';
    return '';
}

/* ===============================
   Load JSON
================================ */

fetch('sections.json')
    .then(res => res.json())
    .then(jsonData => {
        for (const title in jsonData) {
            sections.push({
                title,
                cases: jsonData[title].map(caption => ({
                    gt: `videos_gt/${caption}.webm`,
                    generated: [
                        'videos_gen/',
                        'videos_diff/',
                        'videos_spec/',
                        'videos_visage/'
                    ],
                    caption
                }))
            });
        }
        generateUI();
    })
    .catch(err => console.error(err));
</script>



<section class="YT-AMBISEM">
  <h2 style="text-align: center;">YT-AMBISEM</h2>
  <hr>
  <br>
  

  <!-- ‰∏ãËΩΩÈìæÊé• -->
  <p style="text-align: center; font-size:20px;">We provide a download link for the YT-AMBISEM dataset.</p>
  <br>
  <div class="centered-link">
    <a  href="https://github.com/foleyimmersive/foleyimmersive.github.io/releases/download/V1.0/description_train.txt" 
    class="download-link" 
    download>
    üîó Train.txt ‰∏ãËΩΩÈìæÊé•
 </a>
 <a  href="https://github.com/foleyimmersive/foleyimmersive.github.io/releases/download/V1.0/description_valid.txt" 
    class="download-link" 
    download>
    üîó Valid.txt ‰∏ãËΩΩÈìæÊé•
 </a>
 <a  href="https://github.com/foleyimmersive/foleyimmersive.github.io/releases/download/V1.0/test_batch_process.txt" 
    class="download-link" 
    download>
    üîó Test.txt ‰∏ãËΩΩÈìæÊé•
 </a>

  </div>
  

  </section>



</body>

</html>



